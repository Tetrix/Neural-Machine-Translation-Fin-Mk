{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(str, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):    \n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 2392808 sentence pairs\n",
      "Counting words...\n",
      "Trimmed to 1915404 sentence pairs\n",
      "Counted words:\n",
      "fin 531025\n",
      "mk 340779\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('fin', 'mk', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train, pairs_test = train_test_split(pairs, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "    # sort a list by sequence length\n",
    "    list_of_samples.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    input_seqs, output_seqs = zip(*list_of_samples)\n",
    "    input_seq_lengths = [len(seq) for seq in input_seqs]\n",
    "    output_seq_lengths = [len(seq) for seq in output_seqs]\n",
    "\n",
    "    padding_value = 0\n",
    "    \n",
    "    pad_input_seqs = pad_sequence(input_seqs, batch_first=False, padding_value=padding_value)\n",
    "    pad_output_seqs = []\n",
    "    for i in output_seqs:\n",
    "        padded = i.new_zeros(max(output_seq_lengths) - i.size(0))\n",
    "        pad_output_seqs.append(torch.cat((i, padded.view(-1, 1)), dim=0))\n",
    "    \n",
    "    pad_output_seqs = torch.stack(pad_output_seqs)\n",
    "    pad_output_seqs = pad_output_seqs.permute(1, 0, 2)\n",
    "\n",
    "    return pad_input_seqs, input_seq_lengths, pad_output_seqs, output_seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dictionary_size, hidden_size, dropout_p=0.2, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(dictionary_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=self.num_layers,\n",
    "                          bidirectional=True\n",
    "                         )\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        embedded = self.embedding(pad_seqs).squeeze(dim=2)\n",
    "        packed = pack_padded_sequence(embedded, seq_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        outputs = pad_packed_sequence(outputs)[0]\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1, device=device):\n",
    "        return (torch.zeros(2*self.num_layers, batch_size, self.hidden_size, device=device), torch.zeros(2*self.num_layers, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_dictionary_size, dropout_p=0.2, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(output_dictionary_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=self.num_layers,\n",
    "                          bidirectional=False\n",
    "                         )\n",
    "        self.lin = nn.Linear(hidden_size, output_dictionary_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, hidden, pad_target_seqs=None, teacher_forcing=False):\n",
    "        batch_size = hidden[1].size(1)\n",
    "        prev_word = torch.tensor(SOS_token * np.ones((1, batch_size)), device=device, dtype=torch.int64)\n",
    "        max_length = pad_target_seqs.size(0) if pad_target_seqs is not None else MAX_LENGTH\n",
    "        outputs = []\n",
    "        for t in range(max_length):\n",
    "            prev_word = prev_word.view(1, -1)\n",
    "            output = self.embedding(prev_word)\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "            output = self.softmax(self.lin(output))\n",
    "    \n",
    "            outputs.append(output)\n",
    "\n",
    "            if teacher_forcing:\n",
    "                prev_word = pad_target_seqs[t]\n",
    "            else:\n",
    "                topv, topi = output[0, :].topk(1)\n",
    "                prev_word = topi.squeeze().detach()\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restore_checkpoint == True:\n",
    "    encoder_checkpoint = torch.load('model/encoder_opt', map_location=\"cuda:0\")\n",
    "    decoder_checkpoint = torch.load('model/decoder_opt', map_location=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = Encoder(input_lang.n_words, hidden_size, dropout_p=0.2).to(device)\n",
    "decoder = Decoder(hidden_size, output_lang.n_words, dropout_p=0.2).to(device)\n",
    "\n",
    "if restore_checkpoint == True:\n",
    "    encoder.load_state_dict(encoder_checkpoint['model_state_dict'], )\n",
    "    decoder.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(reduction='none')\n",
    "\n",
    "def compute_loss(decoder_outputs, pad_target_seqs, padding_value=0):  \n",
    "    n = 0\n",
    "    loss = 0\n",
    "    for i in range(decoder_outputs.shape[1]):\n",
    "        p_arr = []\n",
    "        t_arr = []\n",
    "        p = decoder_outputs[:, i, :]\n",
    "        t = pad_target_seqs[:, i].squeeze()\n",
    "        for word in range(len(t)):\n",
    "            if t[word] != padding_value:\n",
    "                p_arr.append(p[word])\n",
    "                t_arr.append(t[word])\n",
    "                n += 1\n",
    "        p_arr = torch.stack(p_arr)\n",
    "        t_arr = torch.stack(t_arr)\n",
    "        loss += criterion(p_arr, t_arr).sum()\n",
    "    loss = loss / n\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restore_checkpoint == True:\n",
    "    encoder_optimizer.load_state_dict(encoder_checkpoint['optimizer'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer'])\n",
    "    \n",
    "    for state in encoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "    for state in decoder_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs = [tensorsFromPair(random.choice(pairs_train)) for i in range(20000)]        \n",
    "pairs_batch_train = DataLoader(dataset=training_pairs,\n",
    "                 batch_size=64,\n",
    "                 shuffle=True,\n",
    "                 collate_fn=collate,\n",
    "                 pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if skip_training == False:\n",
    "    for epoch in range(n_epochs):\n",
    "        testing_pairs = [tensorsFromPair(random.choice(pairs_test)) for i in range(1000)]\n",
    "        pairs_batch_test = DataLoader(dataset=testing_pairs,\n",
    "                 batch_size=64,\n",
    "                 shuffle=True,\n",
    "                 collate_fn=collate,\n",
    "                 pin_memory=True)\n",
    "\n",
    "        complete_loss = 0\n",
    "        for i, batch in enumerate(pairs_batch_train):\n",
    "            pad_input_seqs, input_seq_lengths, pad_target_seqs, target_seq_lengths = batch\n",
    "            batch_size = pad_input_seqs.size(1)\n",
    "            pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "\n",
    "            encoder_hidden = encoder.init_hidden(batch_size, device)\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # Encode input sequence\n",
    "            _, encoder_hidden = encoder(pad_input_seqs, input_seq_lengths, encoder_hidden)\n",
    "            # Decode using target sequence for teacher forcing\n",
    "            decoder_hidden = (encoder_hidden[0].mean(0, keepdim=True),encoder_hidden[1].mean(0, keepdim=True))\n",
    "\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                teacher_forcing = True\n",
    "            else:\n",
    "                teacher_forcing = False\n",
    "                \n",
    "            decoder_outputs, decoder_hidden = decoder(decoder_hidden, pad_target_seqs, teacher_forcing=teacher_forcing)\n",
    "            loss = compute_loss(decoder_outputs, pad_target_seqs, padding_value=0)\n",
    "            loss.backward()\n",
    "            \n",
    "            complete_loss += loss\n",
    "\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "        print('[Epoch: %d] loss: %.4f' % (epoch + 1, complete_loss / len(pairs_batch_train)))\n",
    "        with open(\"loss.txt\", \"a\") as f:\n",
    "            f.write(str(complete_loss.item() / len(pairs_batch_train)) + '\\n')  \n",
    "            \n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(pairs_batch_test):\n",
    "                pad_input_seqs, input_seq_lengths, pad_target_seqs, target_seq_lengths = batch\n",
    "                batch_size = pad_input_seqs.size(1)\n",
    "                pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "\n",
    "                encoder_hidden = encoder.init_hidden(batch_size, device)\n",
    "\n",
    "                # Encode input sequence\n",
    "                _, encoder_hidden = encoder(pad_input_seqs, input_seq_lengths, encoder_hidden)\n",
    "                # Decode using target sequence for teacher forcing\n",
    "                decoder_hidden = (encoder_hidden[0].mean(0, keepdim=True),encoder_hidden[1].mean(0, keepdim=True))\n",
    "\n",
    "                decoder_outputs, decoder_hidden = decoder(decoder_hidden, pad_target_seqs, teacher_forcing=False)\n",
    "                loss = compute_loss(decoder_outputs, pad_target_seqs, padding_value=0)\n",
    "                test_loss += loss\n",
    "\n",
    "            print('[Epoch: %d] test loss: %.4f' % (epoch + 1, test_loss / len(pairs_batch_test)))\n",
    "            with open(\"test_loss.txt\", \"a\") as f:\n",
    "                f.write(str(test_loss.item() / len(pairs_batch_test)) + '\\n')\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:     \n",
    "            state = {'epoch': epoch + 1, 'state_dict': encoder.state_dict(),\n",
    "                 'optimizer': encoder_optimizer.state_dict(), 'model_state_dict': encoder.state_dict()}\n",
    "            torch.save(state, 'model/encoder_opt')\n",
    "\n",
    "            state = {'epoch': epoch + 1, 'state_dict': decoder.state_dict(),\n",
    "                 'optimizer': decoder_optimizer.state_dict(), 'model_state_dict': decoder.state_dict()}\n",
    "            torch.save(state, 'model/decoder_opt')\n",
    "\n",
    "            torch.save(encoder, 'model/encoder')\n",
    "            torch.save(decoder, 'mode/decoder')\n",
    "    \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/15/porjazd1/unix/.local/lib/python3.6/site-packages/torch/serialization.py:401: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/u/15/porjazd1/unix/.local/lib/python3.6/site-packages/torch/serialization.py:401: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "if skip_training == False:\n",
    "    torch.save(encoder, 'model/encoder')\n",
    "    torch.save(decoder, 'model/decoder')\n",
    "else:\n",
    "    encoder = torch.load('model/encoder')\n",
    "    encoder.eval()\n",
    "    \n",
    "    decoder = torch.load('model/decoder')\n",
    "    decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_seq):\n",
    "    with torch.no_grad():\n",
    "        input_length = input_seq.size(0)\n",
    "        batch_size = 1\n",
    "\n",
    "        encoder_hidden = encoder.init_hidden(batch_size, device)\n",
    "        input_seq = input_seq.view(-1, 1, 1).to(device)\n",
    "        encoder_output, encoder_hidden = encoder(input_seq, [input_length], encoder_hidden)\n",
    "\n",
    "        decoder_hidden = (encoder_hidden[0].mean(0, keepdim=True),encoder_hidden[1].mean(0, keepdim=True))\n",
    "        decoder_outputs, decoder_hidden = decoder(decoder_hidden, pad_target_seqs=None, teacher_forcing=False)\n",
    "\n",
    "        output_seq = []\n",
    "        for t in range(decoder_outputs.size(0)):\n",
    "            topv, topi = decoder_outputs[t].data.topk(1)\n",
    "            output_seq.append(topi.item())\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on training data:\n",
      "-----------------------------\n",
      "> totta kai . sinä rakastat kalaa. EOS\n",
      "= да секако ти сакаш риби. EOS\n",
      "< ти си го знаеш и EOS\n",
      "\n",
      "> visuaalisesti erittäin voimakasta. EOS\n",
      "= визуално е много динамично! EOS\n",
      "< и јас EOS\n",
      "\n",
      "> tiedät sen. EOS\n",
      "= го знаеш тоа и сама. EOS\n",
      "< се допаѓа тоа. EOS\n",
      "\n",
      "> pureskelet suussasi lihaa. EOS\n",
      "= тоа е месо што го буташ во уста. EOS\n",
      "< ќе да да го EOS\n",
      "\n",
      "> anteeksi! EOS\n",
      "= извинете за блицот. EOS\n",
      "< се EOS\n",
      "\n",
      "> katso kunnolla. EOS\n",
      "= навистина погледни. EOS\n",
      "< да, EOS\n",
      "\n",
      "> haluat olla rakastettu! EOS\n",
      "= сакаш да бидеш сакан? EOS\n",
      "< како да да EOS\n",
      "\n",
      "> mitä skanneri näkee? EOS\n",
      "= што гледаат скенерите? EOS\n",
      "< што се EOS\n",
      "\n",
      "> se on uusi kortti . unohdin aktivoida sen. EOS\n",
      "= нова е, заборавив да ја активирам. EOS\n",
      "< ова е е е EOS\n",
      "\n",
      "> onko teillä ajokorttia?   on. EOS\n",
      "= имате возачка дозвола? EOS\n",
      "< има е е EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate random sentences from the training set\n",
    "print('\\nEvaluate on training data:')\n",
    "print('-----------------------------')\n",
    "for i in range(10):\n",
    "    input_sentence, target_sentence = training_pairs[np.random.choice(len(training_pairs))]\n",
    "    print('>', ' '.join(input_lang.index2word[i.item()] for i in input_sentence))\n",
    "    print('=', ' '.join(output_lang.index2word[i.item()] for i in target_sentence))\n",
    "    output_sentence = evaluate(input_sentence)\n",
    "    print('<', ' '.join(output_lang.index2word[i] for i in output_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_BLEU(pairs_test):\n",
    "    testing_pairs = [tensorsFromPair(pairs_test[i]) for i in range(int(len(pairs_test)/10))]\n",
    "    bleu_score = 0\n",
    "    for i in range(len(testing_pairs)):\n",
    "        input_sentence, target_sentence = testing_pairs[i]    \n",
    "        reference = []\n",
    "        candidate = []\n",
    "        for word in target_sentence:\n",
    "            if output_lang.index2word[word.item()] != 'EOS':\n",
    "                reference.append(output_lang.index2word[word.item()])\n",
    "\n",
    "        output_sentence = evaluate(input_sentence)\n",
    "        for word in output_sentence:\n",
    "            if output_lang.index2word[word] != 'EOS':\n",
    "                candidate.append(output_lang.index2word[word])\n",
    "\n",
    "        bleu_score += sentence_bleu([reference], candidate)\n",
    "    return bleu_score / int(len(pairs_test)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.857783018922473e-05"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_BLEU(pairs_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
